import json
import re
import sys
import hashlib
import time
from base64 import b64decode, b64encode
from urllib.parse import urlparse, urljoin

import requests
from Crypto.Cipher import AES
from Crypto.Util.Padding import unpad
from pyquery import PyQuery as pq

sys.path.append('..')
from base.spider import Spider as BaseSpider

# å›¾ç‰‡ç¼“å­˜ï¼Œé¿å…é‡å¤è§£å¯†
img_cache = {}

class Spider(BaseSpider):

    def init(self, extend=""):
        try:
            self.proxies = json.loads(extend)
        except:
            self.proxies = {}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        }
        self.host = self.get_working_host()
        self.headers.update({'Origin': self.host, 'Referer': f"{self.host}/"})
        print(f"ä½¿ç”¨ç«™ç‚¹: {self.host}")

    def getName(self):
        return "ğŸŒˆ åƒç“œç½‘|Proå¢å¼ºç‰ˆ"

    def isVideoFormat(self, url):
        return any(ext in (url or '').lower() for ext in ['.m3u8', '.mp4', '.ts', '.flv', '.mkv', '.avi'])

    def manualVideoCheck(self):
        return False

    def destroy(self):
        global img_cache
        img_cache.clear()

    def get_working_host(self):
        dynamic_urls = [
            'https://basic.xhhcfqf.cc/'
        ]
        for url in dynamic_urls:
            try:
                # å‡å°‘è¶…æ—¶æ—¶é—´ï¼ŒåŠ å¿«æ£€æµ‹
                response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=3)
                if response.status_code == 200:
                    return url.rstrip('/')
            except Exception:
                continue
        return dynamic_urls[0].rstrip('/')

    def homeContent(self, filter):
        try:
            response = requests.get(self.host, headers=self.headers, proxies=self.proxies, timeout=10)
            if response.status_code != 200: 
                return {'class': [], 'list': []}
            
            response.encoding = response.apparent_encoding
            data = self.getpq(response.text)
            
            classes = []
            # å¢åŠ é€‰æ‹©å™¨èŒƒå›´
            nav_items = data('nav a, .menu a, .nav a, #header a, .header a, ul.navbar-nav a, .category-list a, .scroll-content a')
            seen_hrefs = set()
            bad_words = ['ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'é¦–é¡µ', 'Home', 'Login', 'Search', 'è”ç³»', 'å…³äº', 'ç•™è¨€', 'RSS', 'æ¨ç‰¹', 'TG', 'Qç¾¤', 'åˆä½œ', 'å…¬å‘Š', 'APP', 'ä¸‹è½½', 'é—®é¢˜', 'å¾€æœŸ', 'ä»£ç†', 'å¯¼èˆª']
            
            for k in nav_items.items():
                href = (k.attr('href') or '').strip()
                name = k.text().strip()
                if not href or href == '#' or href == '/' or 'javascript' in href: continue
                if not name or len(name) < 2 or len(name) > 12: continue # æ”¾å®½é•¿åº¦é™åˆ¶
                if any(bw in name for bw in bad_words): continue
                if href in seen_hrefs: continue
                
                # è§„èŒƒåŒ– href
                if not href.startswith('http'):
                     href = urljoin(self.host, href)
                     
                classes.append({'type_name': name, 'type_id': href})
                seen_hrefs.add(href)
                if len(classes) >= 25: break
            
            if not classes:
                classes = [{'type_name': 'æœ€æ–°', 'type_id': '/latest/'}, {'type_name': 'çƒ­é—¨', 'type_id': '/hot/'}]
            
            videos = self.getlist(data, '#content article, #main article, .posts article, .container .row article, article, .video-list .video-item')
            return {'class': classes, 'list': videos}
        except Exception as e:
            print(f"Home Error: {e}")
            return {'class': [], 'list': []}

    def homeVideoContent(self):
        # å¤ç”¨ homeContent é€»è¾‘ï¼Œå‡å°‘ä»£ç å†—ä½™
        res = self.homeContent(None)
        return {'list': res.get('list', [])}

    def categoryContent(self, tid, pg, filter, extend):
        try:
            if '@folder' in tid:
                v = self.getfod(tid.replace('@folder', ''))
                return {'list': v, 'page': 1, 'pagecount': 1, 'limit': 90, 'total': len(v)}
            
            pg = int(pg) if pg else 1
            url = tid if tid.startswith('http') else f"{self.host}{tid if tid.startswith('/') else '/'+tid}"
            url = url.rstrip('/')
            
            real_url = f"{url}/" if pg == 1 else f"{url}/{pg}/"
            # å…¼å®¹æŸäº›ç«™ç‚¹çš„åˆ†é¡µå‚æ•° ?page=2
            if 'page=' in url or 'pg=' in url:
                 real_url = url.replace('{pg}', str(pg))
                
            response = requests.get(real_url, headers=self.headers, proxies=self.proxies, timeout=10)
            if response.status_code != 200: 
                return {'list': [], 'page': pg, 'pagecount': 9999, 'limit': 90, 'total': 0}
                
            data = self.getpq(response.text)
            videos = self.getlist(data, '#content article, #main article, .posts article, article, .video-list .video-item', tid)
            return {'list': videos, 'page': pg, 'pagecount': 9999, 'limit': 90, 'total': 999999}
        except Exception as e:
            return {'list': [], 'page': pg, 'pagecount': 9999, 'limit': 90, 'total': 0}

    def detailContent(self, ids):
        try:
            url = ids[0] if ids[0].startswith('http') else f"{self.host}{ids[0]}"
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=10)
            html_text = response.text
            data = self.getpq(html_text)
            
            plist = []
            unique_urls = set()

            def add_play_url(name, u):
                if not u or u in unique_urls: return
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if not u.startswith('http'):
                    u = urljoin(self.host, u)
                unique_urls.add(u)
                plist.append(f"{name}${u}")

            # --- 1. åŸå§‹è§„åˆ™ï¼šä¼˜å…ˆåŒ¹é… Script ä¸­çš„ m3u8/mp4 ---
            scripts = data('script')
            for s in scripts.items():
                txt = s.text()
                if 'url' in txt and ('.m3u8' in txt or '.mp4' in txt):
                    # ä¼˜åŒ–æ­£åˆ™ï¼Œé˜²æ­¢åŒ¹é…åˆ° truncated å­—ç¬¦ä¸²
                    urls = re.findall(r'[\"\'](http[^\"\']+\.(?:m3u8|mp4)[^\"\']*)[\"\']', txt)
                    for u in urls:
                        add_play_url("ç²¾é€‰æº", u)
                        break 
            
            # --- 2. åŸå§‹è§„åˆ™ï¼šDPlayer ---
            if data('.dplayer'):
                for c, k in enumerate(data('.dplayer').items(), start=1):
                    config_attr = k.attr('data-config')
                    if config_attr:
                        try:
                            config = json.loads(config_attr)
                            video_url = config.get('video', {}).get('url', '')
                            add_play_url(f"äº‘æ’­{c}", video_url)
                        except: pass

            # --- 3. æ–°å¢é€šç”¨è§„åˆ™ï¼šHTML5 Video æ ‡ç­¾ ---
            for v in data('video').items():
                src = v.attr('src')
                if src: add_play_url("HTML5ç›´è¿", src)
                for src_tag in v('source').items():
                     add_play_url("HTML5æº", src_tag.attr('src'))

            # --- 4. æ–°å¢é€šç”¨è§„åˆ™ï¼šIframe å—…æ¢ ---
            for iframe in data('iframe').items():
                src = iframe.attr('src') or iframe.attr('data-src')
                if src and any(x in src for x in ['.m3u8', '.mp4', 'upload', 'cloud', 'player']):
                    if 'google' not in src and 'facebook' not in src: # æ’é™¤å¸¸è§å¹¿å‘Š
                        add_play_url("äº‘è§£æ", src)

            # --- 5. æ–°å¢é€šç”¨è§„åˆ™ï¼šå¸¸è§å˜é‡/Jsonæ­£åˆ™ (æ ¸å¿ƒå¢å¼º) ---
            # åŒ¹é…å¸¸è§çš„ CMS æ’­æ”¾å™¨é…ç½®å˜é‡
            common_patterns = [
                r'var\s+main\s*=\s*[\"\']([^\"\']+)[\"\']',
                r'url\s*:\s*[\"\']([^\"\']+\.(?:m3u8|mp4))[\"\']',
                r'vurl\s*=\s*[\"\']([^\"\']+)[\"\']',
                r'vid\s*:\s*[\"\']([^\"\']+\.(?:m3u8|mp4))[\"\']',
                r'"url"\s*:\s*"([^"]+)"',
                r'video_url\s*=\s*[\'"]([^\'"]+)[\'"]',
            ]
            for pat in common_patterns:
                if match := re.search(pat, html_text):
                    u = match.group(1)
                    # å¿½ç•¥éè§†é¢‘é“¾æ¥
                    if any(ext in u for ext in ['.m3u8', '.mp4', '.flv', 'http']):
                        add_play_url("é€šç”¨å—…æ¢", u)

            # --- 6. åŸå§‹è§„åˆ™å…œåº•ï¼šæ–‡æœ¬é“¾æ¥ ---
            if not plist:
                content_area = data('.post-content, article, .content, .video-info')
                for i, link in enumerate(content_area('a').items(), start=1):
                    link_text = link.text().strip()
                    link_href = link.attr('href')
                    if link_href and any(kw in link_text for kw in ['ç‚¹å‡»è§‚çœ‹', 'è§‚çœ‹', 'æ’­æ”¾', 'è§†é¢‘', 'ç¬¬ä¸€å¼¹', 'çº¿è·¯']):
                        ep_name = link_text.replace('ç‚¹å‡»è§‚çœ‹ï¼š', '').replace('ç‚¹å‡»è§‚çœ‹', '').strip()
                        if not ep_name: ep_name = f"çº¿è·¯{i}"
                        add_play_url(ep_name, link_href)

            play_url = '#'.join(plist) if plist else f"æ— è§†é¢‘æºï¼Œè¯·å°è¯•ç½‘é¡µæ’­æ”¾${url}"
            
            # æ ‡é¢˜è·å–ä¼˜åŒ–
            vod_title = data('h1').text().strip() 
            if not vod_title: vod_title = data('.post-title').text().strip()
            if not vod_title: vod_title = data('title').text().split('|')[0].strip()
            
            return {'list': [{'vod_play_from': 'åƒç“œç½‘Pro', 'vod_play_url': play_url, 'vod_content': vod_title}]}
        except Exception as e:
            print(f"Detail Error: {e}")
            return {'list': [{'vod_play_from': 'åƒç“œç½‘Pro', 'vod_play_url': 'è·å–å¤±è´¥'}]}

    def searchContent(self, key, quick, pg="1"):
        try:
            pg = int(pg) if pg else 1
            url = f"{self.host}/?s={key}" 
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=10)
            data = self.getpq(response.text)
            return {'list': self.getlist(data, 'article, .search-result, .post, .video-item'), 'page': pg, 'pagecount': 9999}
        except:
            return {'list': [], 'page': pg, 'pagecount': 9999}

    def playerContent(self, flag, id, vipFlags):
        # å¦‚æœæ˜¯ iframe çš„ srcï¼Œé€šå¸¸éœ€è¦ webview è§£æï¼Œflag=1
        # å¦‚æœæ˜¯ç›´æ¥çš„ .m3u8/.mp4ï¼Œflag=0
        if 'html' in id or 'php' in id:
            parse = 1
        elif self.isVideoFormat(id):
            parse = 0
        else:
            parse = 1 # é»˜è®¤è§£æ
        
        url = self.proxy(id) if '.m3u8' in id else id
        return {'parse': parse, 'url': url, 'header': self.headers}

    def localProxy(self, param):
        try:
            type_ = param.get('type')
            url = param.get('url')
            if type_ == 'cache':
                key = param.get('key')
                if content := img_cache.get(key):
                    return [200, 'image/jpeg', content]
                return [404, 'text/plain', b'Expired']
            elif type_ == 'img':
                real_url = self.d64(url) if not url.startswith('http') else url
                # å›¾ç‰‡æ˜¯åŠ å¯†çš„ï¼Œæ‰€ä»¥å¿…é¡»è§£å¯†
                res = requests.get(real_url, headers=self.headers, proxies=self.proxies, timeout=10)
                content = self.aesimg(res.content)
                return [200, 'image/jpeg', content]
            elif type_ == 'm3u8':
                return self.m3Proxy(url)
            else:
                return self.tsProxy(url)
        except:
            return [404, 'text/plain', b'']

    def proxy(self, data, type='m3u8'):
        if data and self.proxies: return f"{self.getProxyUrl()}&url={self.e64(data)}&type={type}"
        return data

    def m3Proxy(self, url):
        url = self.d64(url)
        res = requests.get(url, headers=self.headers, proxies=self.proxies)
        data = res.text
        base = res.url.rsplit('/', 1)[0]
        lines = []
        for line in data.split('\n'):
            if '#EXT' not in line and line.strip():
                if not line.startswith('http'):
                    # ä¿®æ­£ m3u8 ç›¸å¯¹è·¯å¾„æ‹¼æ¥é—®é¢˜
                    if line.startswith('/'):
                         host_base = '/'.join(res.url.split('/')[:3])
                         line = f"{host_base}{line}"
                    else:
                         line = f"{base}/{line}"
                lines.append(self.proxy(line, 'ts'))
            else:
                lines.append(line)
        return [200, "application/vnd.apple.mpegurl", '\n'.join(lines)]

    def tsProxy(self, url):
        return [200, 'video/mp2t', requests.get(self.d64(url), headers=self.headers, proxies=self.proxies).content]

    def e64(self, text):
        return b64encode(str(text).encode()).decode()

    def d64(self, text):
        return b64decode(str(text).encode()).decode()

    def aesimg(self, data):
        if len(data) < 16: return data
        # ä¿ç•™åŸæœ‰çš„å¯†é’¥ï¼Œè¿™æ˜¯è¯¥ç«™ç‚¹ç‰¹æœ‰çš„è§£å¯†é€»è¾‘
        keys = [(b'f5d965df75336270', b'97b60394abc2fbe1'), (b'75336270f5d965df', b'abc2fbe197b60394')]
        for k, v in keys:
            try:
                dec = unpad(AES.new(k, AES.MODE_CBC, v).decrypt(data), 16)
                # å¢åŠ å¯¹å¸¸è§å›¾ç‰‡å¤´çš„æ£€æµ‹
                if dec.startswith(b'\xff\xd8') or dec.startswith(b'\x89PNG') or dec.startswith(b'GIF8'): return dec
            except: pass
            try:
                dec = unpad(AES.new(k, AES.MODE_ECB).decrypt(data), 16)
                if dec.startswith(b'\xff\xd8'): return dec
            except: pass
        return data

    def getlist(self, data_pq, selector, tid=''):
        videos = []
        is_folder = '/mrdg' in (tid or '')
        
        items = data_pq(selector)
        # å¦‚æœé»˜è®¤é€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œå°è¯•å®½æ³›æœç´¢
        if len(items) == 0:
            items = data_pq('a:has(img)')
        
        seen_ids = set()
        ad_keywords = ['å¨±ä¹', 'æ£‹ç‰Œ', 'æ¾³é—¨', 'è‘¡äº¬', 'å¤ªé˜³åŸ', 'å½©ç¥¨', 'AV', 'çº¦ç‚®', 'ç›´æ’­', 'å‘ç‰Œ', 'è·å®˜', 'å¤‡ç”¨', 'å¯¼èˆª', 'å›å®¶', 'è·¯å£', 'APP', 'ä¸‹è½½', 'ç¾¤', 'å……å€¼']

        for k in items.items():
            if k.is_('a'):
                a = k
                container = k.parent() 
            else:
                a = k('a').eq(0)
                container = k

            href = a.attr('href')
            if not href: continue
            
            if any(x in href for x in ['/category/', '/tag/', '/feed/', '/page/', '/author/', 'gitlub', 'homeway', 'faq']):
                continue
            if href == '/' or href.strip() == '#': continue

            title = container.find('h2, h3, .title, .video-title').text()
            if not title: title = a.attr('title')
            if not title: title = a.find('img').attr('alt')
            if not title: title = a.text()
            
            if not title or len(title.strip()) < 2: continue
            if any(ad in title for ad in ad_keywords): continue

            card_html = k.outer_html() if hasattr(k, 'outer_html') else str(k)
            script_text = k('script').text() # æå– script å†…å®¹ç”¨äºæŸ¥æ‰¾å›¾ç‰‡å˜é‡
            
            # ä¼ å…¥ script æ–‡æœ¬ï¼Œç¡®ä¿ getimg èƒ½ä¼˜å…ˆåŒ¹é…åˆ° var img_url
            img = self.getimg(script_text, k, card_html)
            
            if not img: continue
            if '.gif' in img.lower(): continue 
            
            if href in seen_ids: continue
            
            # è¡¥å…¨ href
            if not href.startswith('http'):
                href = urljoin(self.host, href)
                
            seen_ids.add(href)

            remark = container.find('time, .date, .meta, .views, .video-duration').text() or ''

            videos.append({
                'vod_id': f"{href}{'@folder' if is_folder else ''}",
                'vod_name': title.strip(),
                'vod_pic': img,
                'vod_remarks': remark,
                'vod_tag': 'folder' if is_folder else '',
                'style': {"type": "rect", "ratio": 1.33}
            })
            
        return videos

    def getimg(self, text, elem=None, html_content=None):
        # 1. ä¼˜å…ˆåŒ¹é… script ä¸­çš„ var img_url (åƒç“œç½‘ç‰¹è‰²)
        if m := re.search(r'var\s+img_url\s*=\s*[\'"]([^\'"]+)[\'"]', text or ''):
            return self._proc_url(m.group(1))
        
        # 2. åŒ¹é… loadBannerDirect
        if m := re.search(r"loadBannerDirect\('([^']+)'", text or ''):
            return self._proc_url(m.group(1))
            
        if html_content is None and elem is not None:
             html_content = elem.outer_html() if hasattr(elem, 'outer_html') else str(elem)
        if not html_content: return ''

        html_content = html_content.replace('&quot;', '"').replace('&apos;', "'").replace('&amp;', '&')

        # 3. åŒ¹é…æ™®é€š src (æ’é™¤ data:image å ä½ç¬¦)
        # è®¸å¤šå»¶è¿ŸåŠ è½½ä½¿ç”¨ data-src æˆ– data-original
        if m := re.search(r'data-src\s*=\s*[\"\']([^\"\']+)[\"\']', html_content, re.I):
             return self._proc_url(m.group(1))
        if m := re.search(r'data-original\s*=\s*[\"\']([^\"\']+)[\"\']', html_content, re.I):
             return self._proc_url(m.group(1))

        # 4. åŒ¹é… http é“¾æ¥
        if m := re.search(r'(https?://[^"\'\s)]+\.(?:jpg|png|jpeg|webp))', html_content, re.I):
            return self._proc_url(m.group(1))

        if 'url(' in html_content:
            m = re.search(r'url\s*\(\s*[\'"]?([^"\'\)]+)[\'"]?\s*\)', html_content, re.I)
            if m: return self._proc_url(m.group(1))
            
        return ''

    def _proc_url(self, url):
        if not url: return ''
        url = url.strip('\'" ')
        if url.startswith('data:'):
            # å¤„ç† data åè®®
            try:
                _, b64_str = url.split(',', 1)
                raw = b64decode(b64_str)
                # å¦‚æœä¸æ˜¯æ ‡å‡†å›¾ç‰‡å¤´ï¼Œå°è¯• AES è§£å¯†
                if not (raw.startswith(b'\xff\xd8') or raw.startswith(b'\x89PNG') or raw.startswith(b'GIF8')):
                    raw = self.aesimg(raw)
                key = hashlib.md5(raw).hexdigest()
                img_cache[key] = raw
                return f"{self.getProxyUrl()}&type=cache&key={key}"
            except: return ""
            
        if not url.startswith('http'):
            url = urljoin(self.host, url)
        
        # å¼ºåˆ¶æ‰€æœ‰å›¾ç‰‡èµ°ä»£ç†è¿›è¡Œè§£å¯† (ä¿®å¤ç‚¹)
        return f"{self.getProxyUrl()}&url={self.e64(url)}&type=img"

    def getfod(self, id):
        return []

    def getpq(self, data):
        try: return pq(data)
        except: return pq(data.encode('utf-8'))
